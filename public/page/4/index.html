
<!DOCTYPE html>
<!--[if IEMobile 7 ]><html class="no-js iem7"><![endif]-->
<!--[if lt IE 9]><html class="no-js lte-ie8"><![endif]-->
<!--[if (gt IE 8)|(gt IEMobile 7)|!(IEMobile)|!(IE)]><!--><html class="no-js" lang="en"><!--<![endif]-->
<head>
  <meta charset="utf-8">
  <title>Literate Programming</title>
  <meta name="author" content="Steve Klabnik">
  

  <!-- http://t.co/dKP3o1e -->
  <meta name="HandheldFriendly" content="True">
  <meta name="MobileOptimized" content="320">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  

  <link rel="canonical" href="http://blog.steveklabnik.com/page/4/index.html"/>
  <link href="/favicon.png" rel="shortcut icon" />
  <link href="/stylesheets/screen.css" media="screen, projection" rel="stylesheet" type="text/css">
  <script src="/javascripts/modernizr-2.0.js"></script>
  <script src="http://s3.amazonaws.com/ender-js/jeesh.min.js"></script>
  <script src="/javascripts/octopress.js" type="text/javascript"></script>
  <link href="http://feeds.feedburner.com/steveklabnik" rel="alternate" title="Literate Programming" type="application/atom+xml"/>
  <!--Fonts from Google's Web font directory at http://google.com/webfonts -->
<link href='http://fonts.googleapis.com/css?family=PT+Serif:regular,italic,bold,bolditalic' rel='stylesheet' type='text/css'>
<link href='http://fonts.googleapis.com/css?family=PT+Sans:regular,italic,bold,bolditalic' rel='stylesheet' type='text/css'>

</head>

<body  >
  <header><hgroup>
  <h1><a href="/">Literate Programming</a></h1>
  
    <h2>Code is data, data is code. s/data/language/g;</h2>
  
</hgroup>

</header>
  <nav role=navigation><ul role=subscription data-subscription="rss">
  <li><a href="http://feeds.feedburner.com/steveklabnik" rel="subscribe-rss" title="subscribe via RSS">RSS</a></li>
  
</ul>
<form action="http://google.com/search" method="get">
  <fieldset role="site-search">
    <input type="hidden" name="q" value="site:blog.steveklabnik.com" />
    <input class="search" type="text" name="q" results="0" placeholder="Search"/>
  </fieldset>
</form>
<ul role=main-navigation>
  <li><a href="/">Blog</a></li>
  <li><a href="/blog/archives">Archives</a></li>
</ul>

</nav>
  <div id="main">
    <div id="content">
      <div class="blog-index">



  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/08/19/a-word-about-why-whyday-and-hackety-hack.html">A Word About Why, Whyday, and Hackety Hack</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-08-19T00:00:00-04:00" pubdate  data-updated="true" >Aug 19<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>Today is Whyday. A programmer&#8217;s holiday. Today is the day that we
don&#8217;t forget that programming isn&#8217;t all Agile and driven by behavior
and Serious Business. Inside each cubicle, the hacker spirit is trying
to burst free. Today, it will.</p>

<p>Today, I&#8217;m also releasing a prerelease version of Hackety Hack 1.0.
It&#8217;s been a long time coming. The announcement of Whyday got me
thinking about the past year of a world without _why, and the &#8216;your
domain is about to expire&#8217; notices got me thinking about how I&#8217;ve now
been the head of the Hackety Hack project for almost 12 months, too. I
don&#8217;t like thinking about this, because I almost let _why down.</p>

<p>I never met _why. I can&#8217;t really claim to know him, or know what his
intentions were, but I think about him a lot. &#8216;_why&#8217; the persona,
really embodies everything that I love about software. The world owes
_why a great debt, and I&#8217;d like to think that I&#8217;m doing what I can to
repay him.</p>

<p>At least, I am now. I don&#8217;t like to think about roughly 6 or 8 months
of the last year of Hackety Hack; I did an exceptionally poor job of
being a steward. Being alone, having no help, and having a bit of an
identity crisis is an excuse; you can decide if it&#8217;s a good or poor
one. But things have been looking up, and we&#8217;ve got some momentum, I
owe a great debt to Fela for helping make this summer happen.</p>

<p>I really do feel that the Little Coder still has a predicament, and
I&#8217;m almost to the point where they can find some solace. I still feel
that Hackety Hack could become an absolutely invauable project, I&#8217;ve
just got to keep going. I can, I will, and I am. But I wouldn&#8217;t be
able to if so many people didn&#8217;t care, say a nice world, or lend a
hand.</p>

<p>So thanks. And as ashbb would say, &#8220;Let&#8217;s all have fun with Hackety Hack!&#8221;</p>

<p>Happy Whyday.</p>

<p>EDIT: The release is now out. See the announcement <a href="http://hackety-hack.com/posts/happy_whyday">here</a>, and get it
<a href="http://hackety-hack.com/">here</a>.</p>

<p>Thanks!</p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/07/24/a-blip-in-time.html">A Blip in Time</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-07-24T00:00:00-04:00" pubdate  data-updated="true" >Jul 24<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>Sometimes, insight comes from very unlikely places.</p>

<p>I&#8217;m keenly interested in the music industry, and how it will evolve, or
possibly die. I care deeply about music; it&#8217;s had a profound effect on my life
over the years. Even though the entire industry has been screaming that
they&#8217;re dying, I truly believe that music will never die. It&#8217;s bigger than
money. I&#8217;m not exactly sure about the details, even though I have some ideas.</p>

<p>In any case, I was reading about this the other day, and I came across <a href="http://sometimesright.com/2010/05/sir-mick-on-music-profits/">this
quote from Mick Jaggar</a>:</p>

<blockquote><p>But I have a take on that – people only made money out of records for a
very, very small time. When The Rolling Stones started out, we didn’t make any
money out of records because record companies wouldn’t pay you! They didn’t
pay anyone!</p>

<p>Then, there was a small period from 1970 to 1997, where people did get paid,
and they got paid very handsomely and everyone made money. But now that period
has gone.</p>

<p>So if you look at the history of recorded music from 1900 to now, there was
a 25 year period where artists did very well, but the rest of the time they
didn’t.</p></blockquote>

<p>An interesting bit of history, and an interesting perspective from someone who
has some skin in the game, to say the least. But it&#8217;s the last sentence of
that blog post that gives the true insight:</p>

<blockquote><p>Don’t stop at 1900, though. If you think of the entire history of the world,
the notion that you could make an outsized return on making music is a
complete aberration.</p></blockquote>

<p>When I first read this sentence, something clicked inside my head, and I had a
realization: does this apply to everything?</p>

<p>We always think that we&#8217;re moving forward. It&#8217;s called &#8216;progress&#8217; for a
reason. But it&#8217;s kind of obvious why we&#8217;d think this: we&#8217;re the ones living
it! Of course we&#8217;d do know wrong!</p>

<p>But what if we are?</p>

<p>What if we&#8217;re not progressing, but regressing? What if the latest and greatest
isn&#8217;t where things are going&#8230; it&#8217;s just a small blip in the graph?</p>

<p><img src="/images/blip.png" alt="" /></p>

<p>Maybe we&#8217;re actually going down the wrong path, building towards deflation,
and need to turn around and revise our position. Maybe we&#8217;ve misstepped. What
if we&#8217;re doing what we think is right, but it&#8217;s actually wrong? What if we&#8217;re
just on the upswing of some local minima?</p>

<p>I don&#8217;t necessarily mean that, for example, the music industry will cause the
total collapse of everything. But maybe it was a temporary peak. Things will
return back to some even level again, and over the grand sum of history, such
a small deviation will be forgotten.</p>

<p>Then I start to apply this line of reasoning to everything. How can we really
know if we&#8217;re doing better than yesterday?</p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/07/17/what-to-know-before-debating-type-systems.html">What to Know Before Debating Type Systems</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-07-17T00:00:00-04:00" pubdate  data-updated="true" >Jul 17<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>Recently, it was brought up on Proggit that Chris Smith&#8217;s &#8220;What to Know Before
Debating Type Systems&#8221; was no longer online. This is a really great article,
and in an effort to make sure it survives, I&#8217;ve grabbed the <a href="http://web.archive.org/web/20080822101209/http://www.pphsg.org/cdsmith/types.html">archive.org
cache</a> and am &#8216;reprinting&#8217; it here. If you&#8217;re into programming languages,
read this and level up!</p>

<p>EDIT: Chris has placed it back online. You can find it <a href="http://cdsmith.wordpress.com/2011/01/09/an-old-article-i-wrote/">here</a>.</p>

<p>Without further ado, &#8220;What to Know Before Debating Type Systems&#8221;:</p>

<hr />

<h3>What To Know Before Debating Type Systems</h3>

<p>I would be willing to place a bet that most computer programmers have, on
multiple occasions, expressed an opinion about the desirability of certain
kinds of type systems in programming languages. Contrary to popular
conception, that&#8217;s a great thing! Programmers who care about their tools are
the same programmers who care about their work, so I hope the debate rages on.</p>

<p>There are a few common misconceptions, though, that confuse these discussions.
This article runs through those I&#8217;ve encountered that obscure the most
important parts of the debate. My goal is to build on a shared understanding
of some of the basic issues, and help people get to the interesting parts more
quickly.</p>

<h3>Classifying Type Systems</h3>

<p>Type systems are commonly classified by several words, of which the most
common are &#8220;static,&#8221; &#8220;dynamic,&#8221; &#8220;strong,&#8221; and &#8220;weak.&#8221; In this section, I
address the more common kinds of classification. Some are useful, and some are
not.</p>

<h4>Strong and Weak Typing</h4>

<p>Probably the most common way type systems are classified is &#8220;strong&#8221; or
&#8220;weak.&#8221; This is unfortunate, since these words have nearly no meaning at all.
It is, to a limited extent, possible to compare two languages with very
similar type systems, and designate one as having the strong<strong>er</strong> of those
two systems. Beyond that, the words mean nothing at all.</p>

<p>Therefore: I give the following general definitions for strong and weak
typing, at least when used as absolutes:</p>

<ul>
<li><strong>Strong typing:</strong> A type system that I like and feel comfortable with</li>
<li><strong>Weak typing:</strong> A type system that worries me, or makes me feel uncomfortable</li>
</ul>


<p>What about when the phrase is used in a more limited sense? Then strong
typing, depending on the speaker or author, may mean anything on the spectrum
from &#8220;static&#8221; to &#8220;sound,&#8221; both of which are defined below.</p>

<h4>Static and Dynamic Types</h4>

<p>This is very nearly the <em>only</em> common classification of type systems that has
real meaning. As a matter of fact, it&#8217;s significance is frequently under-
estimated. I realize that may sound ridiculous; but this theme will recur
throughout this article. Dynamic and static type systems are two <em>completely</em>
different things, whose goals happen to partially overlap.</p>

<p>A static type system is a mechanism by which a compiler examines source code
and assigns labels (called &#8220;types&#8221;) to pieces of the syntax, and then uses
them to infer something about the program&#8217;s behavior. A dynamic type system is
a mechanism by which a compiler generates code to keep track of the sort of
data (coincidentally, also called its &#8220;type&#8221;) used by the program. The use of
the same word &#8220;type&#8221; in each of these two systems is, of course, not really
entirely coincidental; yet it is best understood as having a sort of weak
historical significance. Great confusion results from trying to find a world
view in which &#8220;type&#8221; really means the same thing in both systems. It doesn&#8217;t.
The better way to approach the issue is to recognize that:</p>

<ul>
<li>Much of the time, programmers are trying to solve the same problem with static and dynamic types.</li>
<li>Nevertheless, static types are not limited to problems solved by dynamic types.</li>
<li>Nor are dynamic types limited to problems that can be solved with static types.</li>
<li>At their core, these two techniques are not the same thing at all.</li>
</ul>


<p>Observing the second of these four simple facts is a popular pass-time in some
circles. Consider <a href="http://web.archive.org/web/20080822101209/http://perl.plover.com/yak/typing/notes.html">this set of presentation notes</a>, with a rather
complicated &#8220;the type system found my infinite loop&#8221; comment. From a
theoretical perspective, preventing infinite loops is in a very deep sense the
most basic possible thing you can do with static types! The simply-typed
lambda calculus, on which all other type systems are based, proves that
programs terminate in a finite amount of time. Indeed, the more interesting
question is how to usefully extend the type system to be able to describe
programs that don&#8217;t terminate! Finding infinite loops, though, is not in the
class of things most people associate with &#8220;types,&#8221; so it&#8217;s surprising. It is,
indeed, provably impossible with dynamic types (that&#8217;s called the halting
problem; you&#8217;ve probably heard of it!). But it&#8217;s nothing special for static
types. Why? Because they are an entirely different thing from dynamic types.</p>

<p>The dichotomy between static and dynamic types is somewhat misleading. Most
languages, even when they claim to be dynamically typed, have some static
typing features. As far as I&#8217;m aware, _all_languages have some dynamic typing
features. However, most languages can be characterized as choosing one or the
other. Why? Because of the first of the four facts listed above: many of the
problems solved by these features overlap, so building in strong versions of
both provides little benefit, and significant cost.</p>

<h4>Other Distinctions</h4>

<p>There are many other ways to classify type systems. These are less common, but
here are some of the more interesting ones:</p>

<ul>
<li><strong>Sound types.</strong> A sound type system is one that provides some kind of guarantee. It is a well-defined concept relating to static type systems, and has proof techniques and all those bells and whistles. Many modern type systems are sound; but older languages like C often do not have sound type systems by design; their type systems are just designed to give warnings for common errors. The concept of a sound type system can be imperfectly generalized to dynamic type systems as well, but the exact definition there may vary with usage.</li>
<li><strong>Explicit/Implicit Types.</strong> When these terms are used, they refer to the extent to which a compiler will reason about the static types of parts of a program. All programming languages have some form of reasoning about types. Some have more than others. ML and Haskell have implicit types, in that no (or very few, depending on the language and extensions in use) type declarations are needed. Java and Ada have very explicit types, and one is constantly declaring the types of things. All of the above have (relatively, compared to C and C++, for example) strong static type systems.</li>
<li><strong>The Lambda Cube.</strong> Various distinctions between static type systems are summarized with an abstraction called the &#8220;lambda cube.&#8221; Its definition is beyond the scope of this article, but it basically looks at whether the system provides certain features: parametric types, dependent types, or type operators. Look <a href="http://web.archive.org/web/20080822101209/http://citeseer.ist.psu.edu/barendregt92lambda.html">here</a> for more information.</li>
<li><strong>Structural/Nominal Types.</strong> This distinction is generally applied to static types with subtyping. Structural typing means a type is assumed whenever it is possible to validly assume it. For example, a record with fields called x, y, and z might be automatically considered a subtype of one with fields x and y. With nominal typing, there would be no such assumed relationship unless it were declared somewhere.</li>
<li><strong>Duck Typing.</strong> This is a word that&#8217;s become popular recently. It refers to the dynamic type analogue of structural typing. It means that rather than checking a tag to see whether a value has the correct general type to be used in some way, the runtime system merely checks that it supports all of the operations performed on it. Those operations may be implemented differently by different types.</li>
</ul>


<p>This is but a small sample, but this section is too long already.</p>

<h3>Fallacies About Static and Dynamic Types</h3>

<p>Many programmers approach the question of whether they prefer static or
dynamic types by comparing some languages they know that use both techniques.
This is a reasonable approach to most questions of preference. The problem, in
this case, is that most programmers have limited experience, and haven&#8217;t tried
a lot of languages. For context, here, six or seven doesn&#8217;t count as &#8220;a lot.&#8221;
On top of that, it requires more than a cursory glance to really see the
benefit of these two very different styles of programming. Two interesting
consequences of this are:</p>

<ul>
<li>Many programmers have used very poor statically typed languages.</li>
<li>Many programmers have used dynamically typed languages very poorly.</li>
</ul>


<p>This section, then, brings up some of the consequences of this limited
experience: things many people assume about static or dynamic typing that just
ain&#8217;t so.</p>

<h4>Fallacy: Static types imply type declarations</h4>

<p>The thing most obvious about the type systems of Java, C, C++, Pascal, and
many other widely-used &#8220;industry&#8221; languages is not that they are statically
typed, but that they are explicitly typed. In other words, they require lots
of type declarations. (In the world of less explicitly typed languages, where
these declarations are optional, they are often called &#8220;type annotations&#8221;
instead. You may find me using that word.) This gets on a lot of people&#8217;s
nerves, and programmers often turn away from statically typed languages for
this reason.</p>

<p>This has nothing to do with static types. The first statically typed languages
were explicitly typed by necessity. However, type inference algorithms -
techniques for looking at source code with no type declarations at all, and
deciding what the types of its variables are - have existed for many years
now. The ML language, which uses it, is among the older languages around
today. Haskell, which improves on it, is now about 15 years old. Even C# is
now adopting the idea, which will raise a lot of eyebrows (and undoubtedly
give rise to claims of its being &#8220;weakly typed&#8221; &#8211; see definition above). If
one does not like type declarations, one is better off describing that
accurately as not liking explicit types, rather than static types.</p>

<p>(This is not to say that type declarations are always bad; but in my
experience, there are few situations in which I&#8217;ve wished to see them
required. Type inference is generally a big win.)</p>

<h4>Fallacy: Dynamically typed languages are weakly typed</h4>

<p>The statement made at the beginning of this thread was that many programmers
have used dynamically typed languages poorly. In particular, a lot of
programmers coming from C often treat dynamically typed languages in a manner
similar to what made sense for C prior to ANSI function prototypes.
Specifically, this means adding lots of comments, long variable names, and so
forth to obssessively track the &#8220;type&#8221; information of variables and functions.</p>

<p>Doing this prevents a programmer from realizing the benefits of dynamic
typing. It&#8217;s like buying a new car, but refusing to drive any faster than a
bicycle. The car is horrible; you can&#8217;t get up the mountain trails, and it
requires gasoline on top of everything else. Indeed, a car is a pretty lousy
excuse for a bicycle! Similarly, dynamically typed languages are pretty lousy
excuses for statically typed languages.</p>

<p>The trick is to compare dynamically typed languages when used in ways that fit
in with their design and goals. Dynamically typed languages have all sorts of
mechanisms to fail immediately and clearly if there is a runtime error, with
diagnostics that show you exactly how it happened. If you program with the
same level of paranoia appropriate to C - where a simple bug may cause a day
of debugging - you will find that it&#8217;s tough, and you won&#8217;t be actually using
your tools.</p>

<p>(As a side comment, and certainly a more controversial one, the converse is
equally true; it doesn&#8217;t make sense to do the same kinds of exhaustive unit
testing in Haskell as you&#8217;d do in Ruby or Smalltalk. It&#8217;s a waste of time.
It&#8217;s interesting to note that the whole TDD movement comes from people who are
working in dynamically typed languages&#8230; I&#8217;m not saying that unit testing is
a bad idea with static types; only that it&#8217;s entirely appropriate to scale it
back a little.)</p>

<h4>Fallacy: Static types imply upfront design or waterfall methods</h4>

<p>Some statically typed languages are also designed to enforce someone&#8217;s idea of
a good development process. Specifically, they often require or encourage that
you specify the whole interface to something in one place, and then go write
the code. This can be annoying if one is writing code that evolves over time
or trying out ideas. It sometimes means changing things in several different
places in order to make one tweak. The worst form of this I&#8217;m aware of (though
done mainly for pragmatic reasons rather than ideological ones) is C and C++
header files. Pascal has similar aims, and requires that all variables for a
procedure or function be declared in one section at the top. Though few other
languages enforce this separation in quite the same way or make it so hard to
avoid, many do encourage it.</p>

<p>It is absolutely true that these language restrictions can get in the way of
software development practices that are rapidly gaining acceptance, including
agile methodologies. It&#8217;s also true that they have nothing to do with static
typing. There is nothing in the core ideas of static type systems that has
anything to do with separating interface from implementation, declaring all
variables in advance, or any of these other organizational restrictions. They
are sometimes carry-overs from times when it was considered normal for
programmers to cater to the needs of their compilers. They are sometimes
ideologically based decisions. They are not static types.</p>

<p>If one doesn&#8217;t want a language deciding how they should go about designing
their code, it would be clearer to say so. Expressing this as a dislike for
static typing confuses the issue.</p>

<p>This fallacy is often stated in different terms: &#8220;I like to do exploratory
programming&#8221; is the popular phrase. The idea is that since everyone knows
statically typed languages make you do everything up front, they aren&#8217;t as
good for trying out some code and seeing what it&#8217;s like. Common tools for
exploratory programming include the REPL (read-eval-print loop), which is
basically an interpreter that accepts statements in the language a line at a
time, evaluates them, and tells you the result. These tools are quite useful,
and they exist for many languages, both statically and dynamically typed. They
don&#8217;t exist (or at least are not widely used) for Java, C, or C++, which
perpetuates the unfortunate myth that they only work in dynamically typed
languages. There may be advantages for dynamic typing in exploratory
programming (in fact, there certainly are <em>some</em> advantages, anyway), but it&#8217;s
up to someone to explain what they are, rather than just to imply the lack of
appropriate tools or language organization.</p>

<h4>Fallacy: Dynamically typed languages provide no way to find bugs</h4>

<p>A common argument leveled at dynamically typed languages is that failures will
occur for the customer, rather than the developer. The problem with this
argument is that it very rarely occurs in reality, so it&#8217;s not very
convincing. Programs written in dynamically typed languages don&#8217;t have far
higher defect rates than programs written in languages like C++ and Java.</p>

<p>One can debate the reasons for this, and there are good arguments to be had
there. One reason is that the average skill level of programmers who know Ruby
is higher than those who know Java, for example. One reason is that C++ and
Java have relatively poor static type systems. Another reason, though, is
testing. As mentioned in the aside above, the whole unit testing movement
basically came out of dynamically typed languages. It has some definite
disadvantages over the guarantees provided by static types, but it also has
some advantages; static type systems can&#8217;t check nearly as many properties of
code as testing can. Ignoring this fact when talking to someone who really
knows Ruby will basically get you ignored in turn.</p>

<h4>Fallacy: Static types imply longer code</h4>

<p>This fallacy is closely associated with the one above about type declarations.
Type declarations are the reason many people associated static types with a
lot of code. However, there&#8217;s another side to this. Static types often allow
one to write much more concise code!</p>

<p>This may seem like a surprising claim, but there&#8217;s a good reason. Types carry
information, and that information can be used to resolve things later on and
prevent programmers from needing to write duplicate code. This doesn&#8217;t show up
often in simple examples, but a really excellent case is found in the Haskell
standard library&#8217;s <code>Data.Map</code> module. This module implements a balanced binary
search tree, and it contains a function whose type signature looks like this:</p>

<blockquote><p>lookup :: (Monad m, Ord k) => k -> Map k a -> m a</p></blockquote>

<p>This is a magical function. It says that I can look something up in a <code>Map</code>
and get back the result. Simple enough, but here&#8217;s the trick: what do I do if
the result isn&#8217;t there? Common answers might include returning a special
&#8220;nothing&#8221; value, or aborting the current computation and going to an error
handler, or even terminating the whole program. The function above does any of
the above! Here&#8217;s how I compare the result against a special nothing value:</p>

<blockquote><p>case (lookup bobBarker employees) of Nothing -> hire bobBarker Just salary
-> pay bobBarker salary</p></blockquote>

<p>How does Haskell know that I want to choose the option of getting back
<code>Nothing</code> when the value doesn&#8217;t exist, rather than raising some other kind of
error? It&#8217;s because I wrote code afterward to compare the result against
<code>Nothing</code>! If I had written code that didn&#8217;t immediately handle the problem
but was called from somewhere that handled errors three levels up the stack,
then <code>lookup</code> would have failed that way instead, and I&#8217;d be able to write
seven or eight consecutive lookup statements and compute something with the
results without having to check for <code>Nothing</code> all the time. This completely
dodges the very serious &#8220;exception versus return value&#8221; debate in handling
failures in many other languages. This debate has no answer. Return values are
great if you want to check them now; exceptions are great if you want to
handle them several levels up. This code simply goes along with whatever you
write the code to do.</p>

<p>The details of this example are specific to Haskell, but similar examples can
be constructed in many statically typed languages. There is no evidence that
code in ML or Haskell is any longer than equivalent code in Python or Ruby.
This is a good thing to remember before stating, as if it were obviously true,
that statically typed languages require more code. It&#8217;s not obvious, and I
doubt if it&#8217;s true.</p>

<h3>Benefits of Static Types</h3>

<p>My experience is that the biggest problems in the static/dynamic typing debate
occur in failing to understand the issues and potential of static types. The
next two sections, then, are devoted to explaining this position in detail.
This section works upward from the pragmatic perspective, while the next
develops it into its full form.</p>

<p>There are a number of commonly cited advantages for static typing. I am going
to list them in order from <em>least</em> to <em>most</em> significant. (This helps the
general structure of working up to the important stuff.)</p>

<h4>Performance</h4>

<p>Performance is the gigantic red herring of all type system debates. The
knowledge of the compiler in a statically typed language can be used in a
number of ways, and improving performance is one of them. It&#8217;s one of the
least important, though, and one of the least interesting.</p>

<p>For most computing environments, performance is the problem of two decades
ago. Last decade&#8217;s problem was already different, and this decades problems
are at least 20 years advanced beyond performance being the main driver of
technology decisions. We have new problems, and performance is not the place
to waste time.</p>

<p>(On the other hand, there are a few environments where performance still
matters. Languages in use there are rarely dynamically typed, but I&#8217;m not
interested enough in them to care much. If you do, maybe this is your corner
of the type system debate.)</p>

<h4>Documentation</h4>

<p>If, indeed, performance is irrelevant, what does one look to next? One answer
is documentation. Documentation is an important aspect of software, and static
typing can help.</p>

<p>Why? Because documentation isn&#8217;t just about comments. It&#8217;s about everything
that helps people understand software. Static type systems build ideas that
help explain a system and what it does. The capture information about the
inputs and outputs of various functions and modules. This is exactly the set
of information needed in documentation. Clearly, if all of this information is
written in comments, there is a pretty good chance it will eventually become
out of date. If this information is written in identifier names, it will be
nearly impossible to fit it all in. It turns out that type information is a
very nice place to keep this information.</p>

<p>That&#8217;s the boring view. As everyone knows, though, it&#8217;s better to have self-
documenting code than code that needs a lot of comments (even if it has
them!). Conveniently enough, most languages with interesting static type
systems have type inference, which is directly analogous to self-documenting
code. Information about the correct way to use a piece of code is extracted
from the code itself (i.e., it&#8217;s self-documenting), but then verified and
presented in a convenient format. It&#8217;s documentation that doesn&#8217;t need to be
maintained or even written, but is available on demand even without reading
the source code.</p>

<h4>Tools and Analysis</h4>

<p>Things get way more interesting than documentation, though. Documentation is
writing for human beings, who are actually pretty good at understanding code
anyway. It&#8217;s great that the static type system can help, but it doesn&#8217;t do
anything fundamentally new.</p>

<p>Fundamentally new things happen when type systems help computer programs to
understand code. Perhaps I need to explain myself here. After all, a wise man
(Martin Fowler, IIRC) one said:</p>

<blockquote><p>&#8220;Any fool can write code that a computer can understand. Good programmers
write code that humans can understand.&#8221;</p></blockquote>

<p>I don&#8217;t disagree with Martin Fowler, but we have different definitions of
<em>understand</em> in mind. Getting a computer to follow code step by step is easy.
Getting a computer to analyze it and answer more complex questions about it is
a different thing entirely, and it is very hard.</p>

<p>We often want our development tools to understand code. This is a big deal.
I&#8217;ll turn back to Martin Fowler, who <a href="http://web.archive.org/web/20080822101209/http://www.martinfowler.com/bliki/DynamicTyping.html">points this out as well</a>.</p>

<h4>Correctness</h4>

<p>Ultimately, though, the justification for static typing has to come back to
writing correct code. Correctness, of course, is just the program doing &#8220;what
you want.&#8221;</p>

<p>This is a really tough problem; perhaps the toughest of all. The theory of
computation has a result called Rice&#8217;s Theorem, which essentially says this:
Given an arbitrary program written in a general purpose programming language,
it is impossible to write a computer program that determines anything about
the program&#8217;s output. If I&#8217;m teaching an intro to programming class and assign
my students to write &#8220;hello world&#8221;, I can&#8217;t program a grader to determine if
they did so or not. There will be some programs for which the answer is easy;
if the program never makes any I/O calls, then the answer is no. If the
program consists of a single print statement, it&#8217;s easy to check if the answer
is yes. However, there will be some complicated programs for which my grader
can never figure out the answer. (A minor but important technical detail: one
can&#8217;t run the program and wait for it to finish, because the program might
never finish!) This is true of any statement about programs, including some
more interesting ones like &#8220;does this program ever finish?&#8221; or &#8220;does this
program violate my security rules?&#8221;</p>

<p>Given that we can&#8217;t actually check the correctness of a program, there are two
approaches that help us make approximations:</p>

<ul>
<li><strong>Testing:</strong> establishes upper bounds on correctness</li>
<li><strong>Proof:</strong> establishes lower bounds on correctness</li>
</ul>


<p>Of course, we care far more about lower bounds than upper bounds. The problem
with proofs, though, is the same as the problem with documentation. Proving
correctness is easy only somewhat insanely difficult when you have a static
body of code to prove things about. When the code is being maintained by three
programmers and changing seven times per day, maintaining the correctness
proofs falls behind. Static typing here plays exactly the same role as it does
with documentation. If (and this is a big if) you can get your proofs of
correctness to follow a certain form that can be reproduced by machine, the
computer itself can be the prover, and let you know if the change you just
made breaks the proof of correctness. The &#8220;certain form&#8221; is called structural
induction (over the syntax of the code), and the prover is called a type
checker.</p>

<p>An important point here is that static typing does not preclude proving
correctness in the traditional way, nor testing the program. It is a technique
to handle those cases in which testing might be guaranteed to succeed so they
don&#8217;t need testing; and similarly, to provide a basis from which the effort of
manual proof can be saved for those truly challenging areas in which it is
necessary.</p>

<h4>Dynamic Typing Returns</h4>

<p>Certainly dynamic typing has answers to this. Dynamically typed languages can
sometimes perform rather well (see Dylan), sometimes have great tools (see
Smalltalk), and I&#8217;m sure they occasionally have good documentation as well,
though the hunt for an example is too much for me right now. These are not
knock-down arguments for static typing, but they are worth being aware of.</p>

<p>The correctness case is particularly enlightening. Just as static types
strengthened our proofs of correctness by making them easier and automatic,
dynamic typing improves testing by making it easier and more effective. It
simply makes the code fail more spectacularly. I find it amusing when novice
programmers believe their main job is preventing programs from crashing. I
imagine this spectacular failure argument wouldn&#8217;t be so appealing to such a
programmer. More experienced programmers realize that correct code is great,
code that crashes could use improvement, but incorrect code that doesn&#8217;t crash
is a horrible nightmare.</p>

<p>It is through testing, then, that dynamically typed languages establish
correctness. Recall that testing establishes only upper bounds on correctness.
(Dijkstra said it best: &#8220;Program testing can be used to show the presence of
bugs, but never to show their absence.&#8221;) The hope is that if one tries hard
enough and still fails to show the presence of bugs, then their absence
becomes more likely. If one can&#8217;t seem to prove any better upper bound, then
perhaps the correctness really is 100%. Indeed, there is probably at some
correlation in that direction.</p>

<h3>What is a Type?</h3>

<p>This is as good a point as any to step back and ask the fundamental question:
what is a type? I&#8217;ve already mentioned that I think there are two answers. One
answer is for static types, and the other is for dynamic types. I am
considering the question for static types.</p>

<p>It is dangerous to answer this question too quickly. It is dangerous because
we risk excluding some things as types, and missing their &#8220;type&#8221; nature
because we never look for it. Indeed, the definition of a type that I will
eventually give is extremely broad.</p>

<h4>Problems with Common Definitions</h4>

<p>One common saying, quoted often in an attempt to reconcile static and dynamic
typing, goes something like this: Statically typed languages assign types to
variables, while dynamically typed languages assign types to values. Of
course, this doesn&#8217;t actually define types, but it is already clearly and
obviously wrong. One could fix it, to some extent, by saying &#8220;statically typed
languages assign types to expressions, &#8230;&#8221; Even so, the implication that
these types are fundamentally the same thing as the dynamic version is quite
misleading.</p>

<p>What is a type, then? When a typical programmer is asked that question, they
may have several answers. Perhaps a type is just a set of possible values.
Perhaps it is a set of operations (a very structural-type-ish view, to be
sure). There could be arguments in favor of each of these. One might make a
list: integers, real numbers, dates, times, and strings, and so on.
Ultimately, though, the problem is that these are all symptoms rather than
definitions. Why is a type a set of values? It&#8217;s because one of the things we
want to prove about our program is that it doesn&#8217;t calculate the square roots
of a string. Why is a type a set of operations? It&#8217;s because one of the things
we want to know is whether our program attempts to perform an impossible
operation.</p>

<p>Let&#8217;s take a look at another thing we often want to know: does our web
application stick data from the client into SQL queries without escaping
special characters first? If this is what we want to know, then these becomes
types. <a href="http://web.archive.org/web/20080822101209/http://blog.moertel.com/articles/2006/10/18/a-type-based-solution-to-the-strings-problem">This article by Tom Moertel</a> builds this on top of Haskell&#8217;s type
system. So far, it looks like a valid definition of &#8220;type&#8221; is as follows:
something we want to know.</p>

<h4>A Type System</h4>

<p>Clearly that&#8217;s not a satisfactory definition of a type. There are plenty of
things we want to know that types can&#8217;t tell us. We want to know whether our
program is correct, but I already said that types provide conservative lower
bounds on correctness, and don&#8217;t eliminate the need for testing or manual
proof. What makes a type a type, then? The other missing component is that a
type is part of a type system.</p>

<p>Benjamin Pierce&#8217;s book <a href="http://web.archive.org/web/20080822101209/http://www.amazon.com/dp/0262162091">Types and Programming Languages</a> is far ans away
the best place to read up on the nitty gritty details of static type systems,
at least if you are academically inclined. I&#8217;ll quote his definition.</p>

<blockquote><p><em>A type system is a tractable syntactic method for proving the absence of
certain program behaviors by classifying phrases according to the kinds of
values they compute.</em></p></blockquote>

<p>This is a complex definition, but the key ideas are as follows:</p>

<ul>
<li><em>syntactic method .. by classifying phrases</em>: A type system is necessarily tied to the syntax of the language. It is a set of rules for working bottom up from small to large phrases of the language until you reach the result.</li>
<li><em>proving the absence of certain program behaviors</em>: This is the goal. There is no list of &#8220;certain&#8221; behaviors, though. The word just means that for any specific type system, there will be a list of things that it proves. What it proves is left wide open. (Later on in the text: &#8220;&#8230; each type system comes with a definition of the behaviors it aims to prevent.&#8221;)</li>
<li><em>tractable</em>: This just means that the type system finishes in a reasonable period of time. Without wanting to put words in anyone&#8217;s mouth, I think it&#8217;s safe to say most people would agree that it&#8217;s a mistake to include this in the definition of a type system. Some languages even have undecidable type systems. Nevertheless, it is certainly a common goal; one doesn&#8217;t expect the compiler to take two years to type-check a program, even if the program will run for two years.</li>
</ul>


<p>The remainder of the definition is mainly unimportant. The &#8220;kinds of values
they compute&#8221; is basically meaningless unless we know what kinds we might
choose from, and the answer is any kind at all.</p>

<p>An example looks something like that. Given the expression <code>5 + 3</code>, a type
checker may look at 5 and infer that it&#8217;s an integer. It may look at 3 and
infer it&#8217;s an integer. It may then look at the + operator, and know that when
+ is applied to two integers, the result is an integer. Thus it&#8217;s proven the
absence of program behaviors (such as adding an integer to a string) by
working up from the basic elements of program syntax.</p>

<h4>Examples of Unusual Type Systems</h4>

<p>That was a pretty boring example, and one that plays right into a trap:
thinking of &#8220;type&#8221; as meaning the same thing it does in a dynamic type system.
Here are some more interesting problems being solved with static types.</p>

<ul>
<li><a href="http://web.archive.org/web/20080822101209/http://wiki.di.uminho.pt/twiki/pub/Personal/Xana/WebHome/report.pdf">http://wiki.di.uminho.pt/twiki/pub/Personal/Xana/WebHome/report.pdf</a>. Uses types to ensure that the correct kinds of data are gotten out of a relational database. Via the type system, the compiler ends up understanding how to work with concepts like functional dependencies and normal forms, and can statically prove levels of normalization.</li>
<li><a href="http://web.archive.org/web/20080822101209/http://www.cs.bu.edu/~hwxi/academic/papers/pldi98.pdf">http://www.cs.bu.edu/~hwxi/academic/papers/pldi98.pdf</a>. Uses an extension to ML&#8217;s type system to prove that arrays are never accessed out of bounds. This is an unusually hard problem to solve without making the languages that solve it unusable, but it&#8217;s a popular one to work on.</li>
<li><a href="http://web.archive.org/web/20080822101209/http://www.cis.upenn.edu/~stevez/papers/LZ06a.pdf">http://www.cis.upenn.edu/~stevez/papers/LZ06a.pdf</a>. This is great. This example uses Haskell&#8217;s type system to let someone define a security policy for a Haskell program, in Haskell, and then proves that the program properly implements that security. If a programmer gets security wrong, the compiler will complain rather than opening up a potential security bug in the system.</li>
<li><a href="http://web.archive.org/web/20080822101209/http://www.brics.dk/RS/01/16/BRICS-RS-01-16.pdf">http://www.brics.dk/RS/01/16/BRICS-RS-01-16.pdf</a>. Just in case you thought type systems only solved easy problems, this bit of Haskell gets the type system to prove two central theorems about the simply typed lambda calculus, a branch of computation theory!</li>
</ul>


<p>The point of these examples is to point out that type systems can solve all
sorts of programming problems. For each of these type systems, concepts of
types are created that represent the ideas needed to accomplish this
particular task with the type system. Some problems solved by static type
systems look nothing like the intuitive idea of a type. A buggy security check
isn&#8217;t normally considered a type error, but only because not many people use
languages with type systems that solve that problem.</p>

<p>To reiterate the point above, it&#8217;s important to understand how limiting it is
to insist, as many people do, that the dynamic typing definition of a &#8220;type&#8221;
is applied to static typing as well. One would miss the chance to solve
several real-world problems mentioned above.</p>

<h4>The True Meaning of Type</h4>

<p>So what is a type? The only true definition is this: a type is a label used by
a type system to prove some property of the program&#8217;s behavior. If the type
checker can assign types to the whole program, then it succeeds in its proof;
otherwise it fails and points out why it failed. This is a definition, then,
but it doesn&#8217;t tell us anything of fundamental importance. Some further
exploration leads us to insight about the fundamental trade-offs involved in
using a static type checker.</p>

<p>If you were looking at things the right way, your ears may have perked up a
few sections back, when I said that Rice&#8217;s Theorem says we can&#8217;t determine
anything about the output of a program. Static type systems prove properties
of code, but it almost appears that Rice&#8217;s Theorem means we can&#8217;t prove
anything of interest with a computer. If true, that would be an ironclad
argument against static type systems. Of course, it&#8217;s not true. However, it is
very nearly true. What Rice&#8217;s Theorem says is that we can&#8217;t determine
anything. (Often the word &#8220;decide&#8221; is used; they mean the same thing here.) It
didn&#8217;t say we can&#8217;t prove anything. It&#8217;s an important distinction!</p>

<p>What this distinction means is that a static type system is a conservative
estimate. If it accepts a program, then we know the program has the properties
proven by that type checker. If it fails&#8230; then we don&#8217;t know anything.
Possibly the program doesn&#8217;t have that property, or possibly the type checker
just doesn&#8217;t know how to prove it. Furthermore, there is an ironclad
mathematical proof that a type checker of any interest at all is <em>always</em>
conservative. Building a type checker that doesn&#8217;t reject any correct programs
isn&#8217;t just difficult; it&#8217;s impossible.</p>

<p>That, then, is the trade-off. We get assurance that the program is correct (in
the properties checked by this type checker), but in turn we must reject some
interesting programs. To continue the pattern, this is the diametric opposite
of testing. With testing, we are assured that we&#8217;ll never fail a correct
program. The trade-off is that for any program with an infinite number of
possible inputs (in other words, any interesting program), a test suite may
still accept programs that are not correct - even in just those properties
that are tested.</p>

<h3>Framing the Interesting Debate</h3>

<p>That last paragraph summarizes the interesting part of the debate between
static and dynamic typing. The battleground on which this is fought out is
framed by eight questions, four for each side:</p>

<ol>
<li>For what interesting properties of programs can we build static type systems?</li>
<li>How close can we bring those type systems to the unattainable ideal of never rejecting a correct program?</li>
<li>How easy can it be made to program in a language with such a static type system?</li>
<li>What is the cost associated with accidentally rejecting a correct computer program?</li>
<li>For what interesting properties of programs can we build test suites via dynamic typing?</li>
<li>How close can we bring those test suites to the unattainable ideal of never accepting a broken program?</li>
<li>How easy can it be made to write and execute test suites for programs?</li>
<li>What is the cost associated with accidentally accepting an incorrect computer program?</li>
</ol>


<p>If you knew the answer to those eight questions, you could tell us all, once
and for all, where and how we ought to use static and dynamic typing for our
programming tasks.</p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/06/30/case-study-in-being-excellent-divvy.html">Case Study in Being Excellent: Divvy</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-06-30T00:00:00-04:00" pubdate  data-updated="true" >Jun 30<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>The other day I wrote a post about <a href="http://blog.steveklabnik.com/being-excellent-to-each-other">being excellent to each other</a>.
Today, I&#8217;ve got a great example of that for you: Divvy.</p>

<p>What would you do if you received an email like this?</p>

<p><img src="/images/divvy1.png" alt="" /></p>

<p>Here&#8217;s what happened:</p>

<p><img src="/images/divvy2.png" alt="" /></p>

<p>Here&#8217;s a <a href="http://www.reddit.com/r/programming/comments/ckcbr/mizage_a_small_development_company_that_made/">link to the thread</a>. Check out some of these comments:</p>

<p><img src="/images/divvy3.png" alt="" /></p>

<p>&#8230; okay, maybe that last one is a bit much. But still, they&#8217;ve obviously been
repaid many times over for something that <em>didn&#8217;t even cost them money</em>. This
<a href="http://news.ycombinator.com/item?id=1473770">comment on HN</a> really says it all:</p>

<p><img src="/images/divvy4.png" alt="" /></p>

<p>If you&#8217;d also like to support a company for being awesome, here&#8217;s a link
to <a href="http://www.mizage.com/divvy/">Divvy</a>.</p>

<p>EDIT: They&#8217;ve posted the sales afterward on Reddit <a href="http://www.reddit.com/r/programming/comments/cmmfg/hey_reddit_a_week_ago_someone_posted_about_divvy/">here</a>.</p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/06/08/productivity-waves.html">Productivity Waves</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-06-08T00:00:00-04:00" pubdate  data-updated="true" >Jun 8<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>Right now, I&#8217;m having a bit of a &#8216;darkness before the dawn&#8217; kind of moment. I
feel like I might soon start to become productive again.</p>

<p>It&#8217;s sort of strange, how these things travel in waves. Just a month or two
ago, I was super-ultra-crazy productive. I was working on Hackety, coding
Bindlr, writing three days a week on the blog, and more. This doesn&#8217;t even
count the more-than-a-full-time CloudFab. But lately, all I&#8217;ve wanted to do
was hang out and play video games.</p>

<p>It seems like this is a cyclic kind of thing. I think 80 hour weeks, even of
things that you love, is kind of unsustainable. It was lots of fun, and I was
feeling pretty good while I was doing it, but now that I&#8217;m off that horse,
it&#8217;s really hard to get back on.</p>

<p>In any case, here&#8217;s the status of some of the stuff I&#8217;ve been up to, if you&#8217;ve
been wondering:</p>

<ul>
<li><p>Blog: I&#8217;ve got a bunch of stuff half-written. I&#8217;ll be starting to finish those
off and getting them up here. I&#8217;ve had lots to say, just no motivation to say it.</p></li>
<li><p>Watch.steve: I want a better design, so I&#8217;m not doing anything with the site
&#8216;till that&#8217;s done. I don&#8217;t want to mess with Ryan&#8217;s look and feel. But I&#8217;ve got
episodes written up, they just need to be recorded.</p></li>
<li><p>Hackety Hack: Fela is making great progress, and I&#8217;m going to start reaching
out to Mac development communities to see if I can get some help on the issue
that&#8217;s ruining my ability to fix things.</p></li>
<li><p>Bindlr: Up, running, and working. Needs more marketing/outreach.</p></li>
<li><p>Twitter: started using it again, you read that post.</p></li>
<li><p>Selling my stuff: most of it is in boxes or thrown away. Time to start selling
the things that were worth money.</p></li>
<li><p>Maria: currently missing her.</p></li>
<li><p>Friends: started actually making it back to XOmB meetings on Saturdays, and
actually going out with some people I haven&#8217;t talked to in a while. I was really
really bad at this in some cases.</p></li>
</ul>


<p>This post is kind of random, and I apologize. I&#8217;ve got a good mix of technical
things, book reviews, and social commentary waiting in the wings.</p>

<p>I&#8217;m excited for what June will bring.</p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/05/01/you-are-terrified-of-your-own-children-since-they-are-natives-in-a-world-where-you-will-always-be-immigrants.html">You Are Terrified of Your Own Children, Since They Are Natives in a World Where You Will Always Be Immigrants.</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-05-01T00:00:00-04:00" pubdate  data-updated="true" >May 1<span>st</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>I just saw this today. It kind of goes along with my &#8220;Move to the Internet&#8221;
post from a few days ago.</p>

<p>Enjoy.</p>

<blockquote><p>Date: Fri, 9 Feb 1996 17:16:35 +0100</p>

<p>To: <a href="mailto:barlow@eff.org">barlow@eff.org</a></p>

<p>From: John Perry Barlow &lt;<a href="mailto:barlow@eff.org">barlow@eff.org</a>></p>

<p>Subject: A Cyberspace Independence Declaration</p>

<p>Yesterday, that great invertebrate in the White House signed into the law
the Telecom &#8220;Reform&#8221; Act of 1996, while Tipper Gore took digital photographs
of the proceedings to be included in a book called &#8220;24 Hours in</p>

<p>Cyberspace.&#8221;</p>

<p>I had also been asked to participate in the creation of this book by writing
something appropriate to the moment. Given the atrocity that this legislation
would seek to inflict on the Net, I decided it was as good a</p>

<p>time as any to dump some tea in the virtual harbor.</p>

<p>After all, the Telecom &#8220;Reform&#8221; Act, passed in the Senate with only 5
dissenting votes, makes it unlawful, and punishable by a $250,000 to say
&#8220;shit&#8221; online. Or, for that matter, to say any of the other 7 dirty words</p>

<p>prohibited in broadcast media. Or to discuss abortion openly. Or to talk
about any bodily function in any but the most clinical terms.</p>

<p>It attempts to place more restrictive constraints on the conversation in
Cyberspace than presently exist in the Senate cafeteria, where I have dined
and heard colorful indecencies spoken by United States senators on every</p>

<p>occasion I did.</p>

<p>This bill was enacted upon us by people who haven&#8217;t the slightest idea who
we are or where our conversation is being conducted. It is, as my good friend
and Wired Editor Louis Rossetto put it, as though &#8220;the illiterate could tell
you what to read.&#8221;</p>

<p>Well, fuck them.</p>

<p>Or, more to the point, let us now take our leave of them. They have declared
war on Cyberspace. Let us show them how cunning, baffling, and powerful we can
be in our own defense.</p>

<p>I have written something (with characteristic grandiosity) that I hope will
become one of many means to this end. If you find it useful, I hope you</p>

<p>will pass it on as widely as possible. You can leave my name off it if you
like, because I don&#8217;t care about the credit. I really don&#8217;t.</p>

<p>But I do hope this cry will echo across Cyberspace, changing and growing and
self-replicating, until it becomes a great shout equal to the idiocy</p>

<p>they have just inflicted upon us.</p>

<p>I give you&#8230;</p>

<p>A Declaration of the Independence of Cyberspace</p>

<p>Governments of the Industrial World, you weary giants of flesh and steel, I
come from Cyberspace, the new home of Mind. On behalf of the future, I ask you
of the past to leave us alone. You are not welcome among us. You have
no sovereignty where we gather.</p>

<p>We have no elected government, nor are we likely to have one, so I address
you with no greater authority than that with which liberty itself always
speaks. I declare the global social space we are building to be naturally
independent of the tyrannies you seek to impose on us. You have no moral
right to rule us nor do you possess any methods of enforcement we have true
reason to fear.</p>

<p>Governments derive their just powers from the consent of the governed. You
have neither solicited nor received ours. We did not invite you. You do not
know us, nor do  you know our world. Cyberspace does not lie within your
borders. Do not think that you can build it, as though it were a public
construction project. You cannot. It is an act of nature and it grows itself
through our collective actions.</p>

<p>You have not engaged in our great and gathering conversation, nor did you
create the wealth of our marketplaces. You do not know our culture, our
ethics, or the unwritten codes that already provide our society more order
than could be obtained by any of your impositions.</p>

<p>You claim there are problems among us that you need to solve. You use this
claim as an excuse to invade our precincts. Many of these problems don&#8217;t
exist. Where there are real conflicts, where there are wrongs, we will
identify them and address them by our means. We are forming our own Social
Contract . This governance will arise according to the conditions of our
world, not yours. Our world is different.</p>

<p>Cyberspace consists of transactions, relationships, and thought itself,
arrayed like a standing wave in the web of our communications.  Ours is a
world that is both everywhere and nowhere, but it is not where bodies live.</p>

<p>We are creating a world that all may enter without privilege or prejudice
accorded by race, economic power, military force, or station of birth.</p>

<p>We are creating a world where anyone, anywhere may express his or her
beliefs, no matter how singular, without fear of being coerced into silence
or conformity.</p>

<p>Your legal concepts of property, expression, identity, movement, and context
do not apply to us. They are based on matter, There is no matter here.</p>

<p>Our identities have no bodies, so, unlike you, we cannot obtain order by
physical coercion. We believe that from ethics, enlightened self-interest, and
the commonweal, our governance will emerge . Our identities may be
distributed across many of your jurisdictions. The only law that all our
constituent cultures would generally recognize is the Golden Rule. We hope we
will be able to build our particular solutions on that basis.  But we
cannot accept the solutions you are attempting to impose.</p>

<p>In the United States, you have today created a law, the Telecommunications
Reform Act, which repudiates your own Constitution and insults the dreams of
Jefferson, Washington, Mill, Madison, DeToqueville, and Brandeis. These dreams
must now be born anew in us.</p>

<p>You are terrified of your own children, since they are natives in a world
where you will always be immigrants. Because you fear them, you entrust your
bureaucracies with the parental responsibilities you are too cowardly
to confront yourselves. In our world, all the sentiments and expressions of
humanity, from the debasing to the angelic, are parts of a seamless whole, the
global conversation of bits. We cannot separate the air that chokes from the
air upon which wings beat.</p>

<p>In China, Germany, France, Russia, Singapore, Italy and the United States,
you are trying to ward off the virus of liberty by erecting guard posts at the
frontiers of Cyberspace. These may keep out the contagion for a small
time, but they will not work in a world that will soon be blanketed in bit-
bearing media.</p>

<p>Your increasingly obsolete information industries would perpetuate
themselves by proposing laws, in America and elsewhere, that claim to own
speech itself throughout the world. These laws would declare ideas to be
another industrial product, no more noble than pig iron. In our world,
whatever the human mind may create can be reproduced and distributed
infinitely at no cost. The global conveyance of thought no longer requires
your factories to accomplish.</p>

<p>These increasingly hostile and colonial measures place us in the same
position as those previous lovers of freedom and self-determination who had to
reject the authorities of distant, uninformed powers. We must declare our
virtual selves immune to your sovereignty, even as we continue to consent to
your rule over our bodies. We will spread ourselves across the Planet so that
no one can arrest our thoughts.</p>

<p>We will create a civilization of the Mind in Cyberspace. May it be more
humane and fair than the world your governments have made before.</p>

<p>Davos, Switzerland</p>

<p>February 8, 1996</p>

<hr />

<p>John Perry Barlow, Cognitive Dissident</p>

<p>Co-Founder, Electronic Frontier Foundation</p>

<p>Home(stead) Page: <a href="http://www.eff.org/~barlow">http://www.eff.org/~barlow</a></p>

<p>Message Service: 800/634-3542</p>

<p>Barlow in Meatspace Today (until Feb 12): Cannes, France</p>

<p>Hotel Martinez: (33) 92 98 73 00, Fax: (33) 93 39 67 82</p>

<p>Coming soon to: Amsterdam 2/13-14, Winston-Salem 2/15, San Francisco</p>

<p>2/16-20, San Jose 2/21, San Francisco 2/21-23, Pinedale, Wyoming</p>

<p>In Memoriam, Dr. Cynthia Horner and Jerry Garcia</p>

<hr />

<p>It is error alone which needs the support of government.  Truth can</p>

<p>stand by itself.</p>

<p>                         &#8211;Thomas Jefferson, Notes on Virginia</p></blockquote>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/04/27/a-break-with-the-past.html">A Break With the Past</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-04-27T00:00:00-04:00" pubdate  data-updated="true" >Apr 27<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p><a href="http://www.countdowntooauth.com/">Pretty soon</a>, Twitter is going to turn off Basic Authentication and switch
entirely to OAuth. <a href="http://www.scripting.com/stories/2010/04/26/theToxicCoralReef.html">People are upset</a>. It&#8217;s natural. If apps aren&#8217;t updated,
they&#8217;ll stop working, entirely. This could be bad.</p>

<p>But in the long run, it won&#8217;t be.</p>

<p>In any sort of long-term endeavor, things can go sour, due to accumulating
cruft over time. You see this in married couples that argue every day, you see
this in old-guard companies that have six layers of management, you see this
in software of all kinds. You don&#8217;t want to break old stuff, so you support
it. Rocking the boat may cause some problems. It could bring up old wounds.
You might have to ask someone to improve. Things won&#8217;t ever be the same, and
you&#8217;re gambling with the outcome, hoping that in the end, everything will be
better.</p>

<p>Sometimes, this gamble pays off big.</p>

<p>Apple&#8217;s managed to do this three times in its history, in a huge way. They&#8217;ve
reinvented themselves a few times, but there were three huge, backward-
compatibility breaking changes that could have killed the company. The first
was the transition from the 68k architecture to the PowerPC architecture, the
eventual change from PowerPC to x86, and the transition from OS9 to OSX.</p>

<p>Apple managed to make all of these transitions successfully, and it&#8217;s been one
of the greatest benefits to the company. A huge part of why Windows is so
horrible is that it goes above and beyond the call of duty with backwards
compatibility; Microsoft never wants to rock the boat. When they try to, they
get punished. Look at how poorly the transition to Vista went. So what made it
go so right for Apple, and so wrong for Microsoft?</p>

<p>The first thing that Apple did right with all of these transitions was being
open about them. People were given ample time to move over to the new
platform. I was young when the 68k change happened, so my remembering of that
time might be fuzzy, but the OSX announcement certainly was made with ample
time. An entire year elapsed between the public beta and the release of 10.1,
which was widely considered the first version of OSX that was worth using. I
remember buying a copy of 10.0, and Apple gave you an upgrade to 10.1 for $30,
similar to the Snow Leopard upgrade. 10.0 was just too buggy to be considered
a proper replacement. Anyway, that was a year, and that&#8217;s not counting the
time between the announcement and the beta release. For the transition to
Intel, the initial announcement was made in 2005, and Steve Jobs said the
transition would happen over the next two years. That gave everyone plenty of
time to get their ducks in a row.</p>

<p>The second thing that Apple did correctly was provide ample tools for dealing
with the transition. For the first switch, they provided a 68k emulator, and
kept it going all the way up until the Intel transition. This meant that
people didn&#8217;t have to re-write their apps from scratch, and gave them a big
window for re-writing apps. Rosetta fulfills the same role for the
PowerPC/Intel change. And during the OS9/OSX changeover, Apple not only let
you emulate OS9, but also created the Carbon framework to bridge the gap
between System 9 and Cocoa.</p>

<p>Finally, Apple made these changes boldly, and didn&#8217;t back down. With any kind
of massive change like this, unless someone forces the change through, people
will stay with the old system, and the transition fails. You can see this
happen with both the Python move to Python 3000, the move from Ruby 1.8 to
1.9, and the move from XP to Vista. There&#8217;s a fine line between giving people
leeway to make the transition and actually forcing them to do it. Apple did a
poor job with this when it came to Adobe, who&#8217;s being an incredible laggard
with Photoshop. It&#8217;s a massive application, sure, but Apple announced Carbon&#8217;s
end of life ages ago, they really should have gotten their shit together.
Generally, Apple&#8217;s not afraid to end-of-life products after the transition
time is over, though. </p>

<p>In any case, those seem to be the three things that make a transitional period
work: transparency, tools, and finality. If Twitter wants to survive the
Oauthcalypse (as if they won&#8217;t), they need to follow the same path. And they
have been. They announced the move to Oauth over a year ago (I think), they&#8217;ve
had libraries and examples out there for ages, and they&#8217;ve picked a date and
they&#8217;re sticking to it.</p>

<p>I&#8217;m glad they&#8217;re doing it. OAuth is an important part of Twitter&#8217;s future, and
it&#8217;s good that they&#8217;re embracing it fully.</p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/04/24/living-in-the-cloud.html">Living in the Cloud</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-04-24T00:00:00-04:00" pubdate  data-updated="true" >Apr 24<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>I&#8217;ve been joking for a while that I&#8217;d like to &#8220;move to the Internet.&#8221;
Generally I cite my familiarity and love with online culture as opposed to
American culture. I don&#8217;t really care what&#8217;s going on in &#8220;the real world,&#8221; as
I feel it&#8217;s kind of played out. The whole cycle of &#8220;fight wars, reduce
freedoms, sit in front of your TV&#8221; has gotten really boring quickly. I find
Internet culture to be more complex, entertaining, and useful. But I&#8217;m getting
way off topic, here.</p>

<p>As a programmer, one of the key skills I rely on daily is the ability to
recognize abstractions. Writing great code requires the ability to see
similarities in things, even when they may not be immediately apparent.
Lately, I&#8217;ve been giving a lot of thought to the idea of &#8220;living in the
cloud.&#8221; Primarily, cloud computing is about providing services on-demand, and
in the amount you need. Dynamic allocation. It&#8217;s generally great stuff, and
even though &#8220;the cloud&#8221; is a bit buzzwordy, it&#8217;s an awesome concept.</p>

<p>So what happens when we apply that to meatspace? What does &#8220;cloud living&#8221; look
like? &#8220;Cloud working&#8221;?</p>

<p>Living in the cloud would look a lot like the world Tim Ferriss encourages
with his whole lifestyle design concept, or at least the lifestyle he&#8217;s
designed for himself. Move around from place to place, live where you find
things interesting, reallocate yourself to a better place when the feeling
dictates. The concept of being a &#8220;digital nomad&#8221; certainly makes me think of
&#8220;living in the cloud.&#8221; Doing this right means giving up most material
possessions, as they impede freedom of movement.</p>

<p>Another important aspect of cloud living would be finding a way to make a
living while being location independent. There was a great article making the
rounds a while back called &#8221;<a href="http://romansnitko.posterous.com/jobs-dont-scale">Jobs don&#8217;t scale</a>.&#8221; It&#8217;s only tangentially
related, but his point about working for other people certainly relates to
freedom of movement. If you have to convince your boss to let you work
remotely, it&#8217;s much harder than just doing it yourself.</p>

<p>In any case, I&#8217;m very attracted to all of this as an idea. I&#8217;d love to travel
the world, working from a different place every day, seeing new sights and
exploring new places. It requires re-thinking a lot of things about what it
means to work, how relationships work, friendships. Can you have kids if you
keep moving around from place to place? How can you stay connected to friends
if you never see them, because you&#8217;re halfway across the globe? Can your
significant other do the same thing? Can you get month to month leases easily?</p>

<p>I&#8217;m still thinking about this as a concept, but I thought I&#8217;d share my
thoughts so far. I&#8217;m not moving around the world yet, but I am trying to
increase my own mobility as much as possible. I think I&#8217;m finally at a place
where I don&#8217;t care much for most of my possessions anymore&#8230; </p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/03/10/book-review-rework.html">Book Review: Rework</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-03-10T00:00:00-05:00" pubdate  data-updated="true" >Mar 10<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>I&#8217;ve been waiting for this book for a while. &#8220;Rework&#8221; is the new book by Jason
Fried and David Heinemeier Hansson from 37signals. It hit stores on Tuesday.
Here&#8217;s a (non-affiliate) link to <a href="http://www.amazon.com/gp/product/0307463745/ref=s9_simh_gw_p14_t1?pf_rd_m=ATVPDKIKX0DER&amp;pf_rd_s=center-2&amp;pf_rd_r=1RRQJN39HSJ4SMVA2EM4&amp;pf_rd_t=101&amp;pf_rd_p=470938631&amp;pf_rd_i=507846">Rework on Amazon</a>.</p>

<p>For those of you who don&#8217;t know, I&#8217;m down in Houston, Texas. I&#8217;m here for a
few days before heading over to Austin for SXSW. There&#8217;s a lot of really cool
start up stuff, as well as 3D people, and a friend of mine. It&#8217;s been a good
trip so far. But even with waking up at 3am, connecting flights, and a few
hours of driving, I still managed to find a spare moment to head over to a
Borders and grab a copy of Rework. And even though I&#8217;m running all over town,
Nick is driving, so I&#8217;ve been able to read all of Rework in between lunches
and networking events.</p>

<p>Rework is interesting. I described it earlier today <a href="http://twitter.com/steveklabnik/status/10281523422">as a philosophy text</a>,
and I feel that description is absolutely apt. It&#8217;s 37signals in its purest,
most potent form. If you&#8217;ve read <a href="http://gettingreal.37signals.com/">Getting Real</a>, this territory will
be familiar. In fact, a lot of it is basically the same. I&#8217;d be lying if I
told you otherwise. The real truth is that it doesn&#8217;t matter. People who don&#8217;t
already know and love 37signals won&#8217;t have read Getting Real, and so this
stuff will be novel to them. People who do won&#8217;t mind re-reading this
information again, as they&#8217;ve bought into the philosophy. And an update isn&#8217;t
a bad thing, either. What makes Rework interesting is how it&#8217;s different from
Getting Real, not what&#8217;s the same.</p>

<p>I thought it&#8217;d be most interesting to talk about Rework in the context of it&#8217;s
own philosophy. I think there are three points in particular in the book
itself that point out just why this book is so good. The first is an old
37signals standby, the other two are new.</p>

<h3>Build half a product, not a half-assed product</h3>

<p>This got <a href="http://gettingreal.37signals.com/ch05_Half_Not_Half_Assed.php">a section devoted to it in Getting Real</a>. Here&#8217;s the core idea:</p>

<blockquote><p>Throw in every decent idea that comes along and you&#8217;ll just wind up with a
half-assed version of your product. What you really want to do is build half a
product that kicks ass.</p></blockquote>

<p>They mention that Rework used to be twice as big. The next to last draft was
57,000 words, and the final draft was 27,000. This is the biggest difference
between the two books. It&#8217;s the most pure dosage of Kool-Aid I&#8217;ve ever read.
Each section feels finely honed. They&#8217;re all either one, two, or three pages,
and an accompanying picture. This book is about what&#8217;s worked for the company
so far over its lifetime, and this refinement process is clearly demonstrated
here.</p>

<p>It&#8217;s always easy to ask for more. I&#8217;m really curious about the things that
were cut. Were there more sections? Was each section twice as long? A little
of both? At the same time, this exactly exemplifies the thinking this section
is railing against. If the book was twice as long, would I have learned twice
as much? Probably not. YAGNI.</p>

<h3>Decommoditize your product</h3>

<blockquote><p>Make you part of your product or service. Inject what&#8217;s unique about the way
you think into what you sell.</p></blockquote>

<p>This is something that these guys do really well. It&#8217;s part of having such
strong opinions, and sharing them with the world. Everyone knows who 37signals
is and what they stand for. If I wrote each chapter of Rework into a blog
post, you&#8217;d still recognize it as their philosophy. It also comes through in
the writing. They mention in the notes that Matthew Linderman helped them pull
their distinct styles of writing into a more cohesive whole. He did a good
job, and didn&#8217;t let the voice get lost in the editing.</p>

<h3>Out-teach your competition</h3>

<blockquote><p>Teaching is something individuals and small companies can do that big
companies can&#8217;t.</p></blockquote>

<p>Teaching is a topic I&#8217;ve been getting more and more into lately. Hackety Hack
is about teaching; when I speak, I&#8217;m teaching; this blog is about teaching.
Rework is about teaching the lessons 37signals have learned about business to
the world. A lot of Signal vs. Noise is about teaching. It&#8217;s a great way to
get people to recognize you, and a great way to give back. The world can
always use more great teachers.</p>

<h2>Wrapping up</h2>

<p>There&#8217;s a reason I&#8217;m up late, writing this review. I couldn&#8217;t put Rework down.
I enjoyed revisiting the old topics; the new ones are still tumbling around in
my brain. I&#8217;m sure this book is going to spawn a bunch of posts on this blog
in the future, as I add my own thoughts to the stuff I&#8217;ve learned.
I&#8217;d recommend this book to anyone who&#8217;s working in the business world or doing
a startup, even if they hate that term. It&#8217;ll give you a lot of interesting
thoughts to chew on.</p>
</div>
  
  


  </article>


  <article>
    
  <header>
    
      <h1 class="entry-title"><a href="/2010/03/08/create-a-more-compelling-experience-for-your-users-through-game-mechanics.html">Create a More Compelling Experience for Your Users Through Game Mechanics</a></h1>
    
    
      <p class="meta">





  



<time datetime="2010-03-08T00:00:00-05:00" pubdate  data-updated="true" >Mar 8<span>th</span>, 2010</time></p>
    
  </header>


  <div class="entry-content"><p>Ever wonder why some websites are so addictive? Certain sites always keep you
going back, time after time after time. Well, I can&#8217;t speak for all of them,
but there&#8217;s a subtle reason that some sites draw your attention on such a
repeated basis: They&#8217;re actually games.</p>

<h2>Wait, games?</h2>

<p>Try a little thought experiment: If I say, &#8220;Yeah, he&#8217;s a ______ addict,&#8221; what
are the first few things that pop into your mind? For me, top two are &#8220;heroin&#8221;
and &#8220;World of Warcraft.&#8221; I&#8217;m not sure what that says about me as a person, but
ignore that for now. What makes these two things so addicting? Why are they
basically synonymous with the word &#8220;addict&#8221;? Lots of people smoke pot. Lots of
people play Call of Duty. Lots do both, and in copious amounts. So why don&#8217;t
they get the same label?</p>

<h2>Heroin: it&#8217;s a hell of a drug</h2>

<p>Yeah, that reference is to cocaine, another famously addictive substance. Oh
well.</p>

<p>Heroin is the poster child for addiction because it&#8217;s got a built-in viral
loop. That sentence sounds ridiculous, but it&#8217;s true. It&#8217;s very easy to start
out with, as it&#8217;s snorted. No scary needles or anything. You get high really
quickly, due to its chemical properties combined with the fact that your nose
is already close to your brain. It gives a really intense high that is also
fairly short. As you do it, you develop both a psychological addiction as well
as a tolerance. You simultaneously develop a deep desire for more of the drug
as you need a larger quantity of the drug to get the same high. Eventually, it
becomes more and more difficult, but you&#8217;re so addicted that you get over your
fear of needles and start mainlining.</p>

<p>World of Warcraft works the same way. It&#8217;s easy to try, as there are
mechanisms to invite your friends, and the system requirements are fairly low
for a video game. The first few quests are super easy, and so you hit that
quick reward. You get addicted to &#8220;Ding!&#8221; but it takes longer and longer every
time you do it. Eventually, you max out on levels and have to start doing
other things to get your fix. It may sound funny, but it&#8217;s absolutely true.
People talk about &#8220;relapsing.&#8221; They speak of &#8220;craving.&#8221; That&#8217;s why WoW has so
many subscribers.</p>

<h2>How to replicate this success</h2>

<p>I can&#8217;t guarantee that you&#8217;ll be able to make your site as addictive as heroin
is, but many sites use the same basic psychology to keep you coming back. Game
mechanics are one of the tools they use to develop that psychological
addiction. This is something we&#8217;ve been seeing more and more of lately, but it
isn&#8217;t really being talked about explicitly as a major trend. I really think
that this stuff is really important and useful.</p>

<p>There are a couple of different mechanisms that web sites can incorporate that
fall under the realm of &#8220;game mechanics:&#8221;</p>

<ul>
<li>Collectibles: <em>Any sort of item you can accumulate. Sometimes comes in &#8220;sets,&#8221; which are finite lists.</em></li>
<li>Points: A concrete number that lets you compare two people. </li>
<li>Levels: A target number of points, you gain the &#8220;level&#8221; when you go over that number.</li>
<li>Trophies: A special kind of level that&#8217;s unrelated to points. You get it for some other arbitrary reason.</li>
<li>Rankings: A place where you can go to see how many points, levels, and trophies others have</li>
<li>Tournaments: A competition between people.</li>
</ul>


<p>We&#8217;ve all heard these terms used in games. But in web sites? Okay, let&#8217;s try
those things again:</p>

<ul>
<li>Collectibles: Gowalla items. Facebook &#8220;Gifts&#8221;</li>
<li>Points: Twitter followers. Facebook friends. Number of feedbacks. Reddit Karma.</li>
<li>Levels: eBay &#8220;Power Sellers.&#8221; Foursquare &#8220;Super Users.&#8221;</li>
<li>Trophies: Badges, of any kind. &#8220;Achievements&#8221;</li>
<li>Rankings: FourSquare&#8217;s Leaderboard. Klout. Listorious. Hacker News&#8217; top list.</li>
<li>Tournaments: I actually can&#8217;t come up with a good example of this. Thoughts?</li>
</ul>


<p>The same feedback loop happens on these websites. You say something
interesting on Twitter, you gain another follower or two. You say something
else, another follower. You check in, oh look, you&#8217;re the mayor! You sell an
extra hundred things and get your Power Seller discount.</p>

<p>That&#8217;s the hard stuff. It&#8217;ll get you hooked, and coming back for more.</p>

<h2>Where&#8217;s all of this going?</h2>

<p>This is the current stuff that&#8217;s being done with game mechanics. But where
could we go, in the future?</p>

<p>A while back, there was a huge debacle over ReadWriteWeb and Facebook connect.
To give you the basic idea, <a href="http://www.readwriteweb.com/">ReadWriteWeb</a> is a blog that talks about
everything Web2.0. They wrote an article entitled &#8221;<a href="http://www.readwriteweb.com/archives/facebook_wants_to_be_your_one_true_login.php">Facebook Wants to be your
One True Login</a>.&#8221; Read the comments. Notice something funny? Due to some
Google magic, if you were to Google &#8220;Facebook login&#8221; the day that was posted,
that article would appear at the top under the &#8220;Google News&#8221; results. Now, RWW
uses Facebook Connect for their commenting system, and a ton of people
apparently don&#8217;t know how to use the Internet. So when they said, &#8220;Hey, I
think I&#8217;ll go to Facebook today,&#8221; they Googled &#8220;facebook login,&#8221; clicked the
news story, and went to RWW. They then ignored that RWW is a blog completely
covered in red that looks nothing like Facebook, scrolled until they found the
Facebook icon, clicked it, logged in, and then said &#8220;wtf, this isn&#8217;t my
facebook? Why&#8217;d they change the interface again???&#8221; This happened a week after
a middle-sized interface upgrade on Facebook, for extra hilarity.</p>

<p>Now, I won&#8217;t comment on those people or that situation directly. But one of my
favorite Hacker News posters, <a href="http://news.ycombinator.com/user?id=patio11">patio11</a>, posted <a href="http://news.ycombinator.com/item?id=1119186">a really interesting
comment</a> about the situation. I&#8217;m linking to the person he&#8217;s responding to,
for context:</p>

<blockquote><p><em>Pyre</em>: Facebook can&#8217;t improve their interface to make users not type
&#8220;facebook login&#8221; into Google as a way of accessing their site.</p>

<p><em>patio11</em>: That is a failure of the imagination. They certainly could &#8211;
whether it is worth doing or not is another question, but hey, that is what
God gave us A/B testing to figure out. </p>

<p>&#8220;Hey user, it looks like you came to us today from Google searching for
[Facebook login]. Did you know that there is a better way? Type
<a href="http://facebook.com">facebook.com</a> into [blah blah blah]. Try it now and we&#8217;ll give you 5 free
credits for [without loss of generality: FarmVille]!&#8221; </p>

<p>Great job! You should do that every time. If you do that to log into
Facebook the next five days you use the service, we&#8217;ll award you a Facebook
Diploma and give you another 10 free credits for [without loss of generality:
FarmVille]!&#8221; </p>

<p>On the back end, you show the above prompts to N% of your users who you
detect coming to the login page from Google search results (this is trivial &#8211;
check the referer). You then compare any user metric you want for the &#8220;Was
Shown Facebook Login Course&#8221; population and &#8220;Complete Facebook Login Course&#8221;
population with the population at large. Kill the test if it hurts your
metrics, deploy it sitewide if it helps them. </p></blockquote>

<p>How cool would that be? Now the game mechanics aren&#8217;t being used just to
increase engagement, but to actually teach people how to use your site or
service. It&#8217;s classical conditioning; reward people for doing the right thing,
and they&#8217;ll keep doing the right thing.</p>

<h2>Game mechanics are your MVP</h2>

<p>So how&#8217;s this stuff relevant to your startup? Well, I think this idea ties in
really well with the concept of a Minimum Viable Product. Here&#8217;s the idea:
Build your MVP, and then build game mechanics in. Unlock new features based on
game mechanics. This gives you a few advantages:</p>

<ol>
<li>Your new users only get the most basic experience, which is still useful. It&#8217;s a simplified, streamlined experience.</li>
<li>Users only get the new features added that are relevant to how they use the site itself.</li>
<li>You can &#8220;fake it till you make it&#8221; by implementing the features that are most useful to your users. Is everyone getting Badge A and not Badge B? Implement Feature A Level 2 first!</li>
</ol>


<p>I think that this makes for a really strong experience, if done right.
Foursquare kind of does this already in a crude way with their Super User
features. But I think it could be taken to a whole new level.</p>

<p>Think about this:  Facebook, where you can only friend people, update your
profile, and send messages at first. Soon you unlock the ability to use
applications. Then the ability to create pages and groups. The interface
slowly unfolds in front of you. What about Reddit, where posting comments is
all you can do at first? A hundred upvotes gives you the ability to downvote.
Ten comments lets you post stories. (Hacker News sort of does this already,
with a minimum karma before downvoting is enabled.)</p>

<p>If you could pull it off, I think it&#8217;d make for a really compelling user
experience. It does bring one extra design skill that many people may not
have, though: balance. Game designers are used to this already, but your
potential &#8220;Power Users&#8221; might not like having to wait to get more advanced
features. Then again, this might also solve some issues, like spam. If you had
to have 100 positively moderated comments before posting a story on Digg, it&#8217;d
be much harder to just sign up for spam accounts to submit bogus stories.</p>

<p>This idea can be taken in a lot of different directions. I&#8217;m sure I&#8217;m only
barely scratching the surface with this idea, but I think it&#8217;ll go really far.
What do you think? Any interesting mechanics I&#8217;ve missed? Any really
interesting thoughts for how services can incorporate game mechanics? I&#8217;ve
decided to re-open comments, but if nobody uses them, I&#8217;ll just shut them off
again. Let me know what you think.</p>

<p>This post has been featured on the Startup Pittsburgh blog,
<a href="http://startuppittsburgh.com/2010/04/create-a-more-compelling-experience-for-your-users-through-game-mechanics/">here</a>.</p>
</div>
  
  


  </article>

<nav role="pagination">
  <div>
    
      <a class="prev" href="/page/5/">&larr; Older</a>
    
    <a href="/blog/archives">Blog Archives</a>
    
    <a class="next" href="/page/3/">Newer &rarr;</a>
    
  </div>
</nav>

</div>
<aside role=sidebar>
  
    <aside>
<img src='http://en.gravatar.com/userimage/5335489/ee56a7574df33ed8748160494c930b98.jpg?size=190' />
<h4>Hi there, I&#8217;m Steve.</h4>
<p>
I write both code and prose. Here&#8217;s some of my thoughts about software,
literature, art and code, with some politics thrown in on occasion.
You might also enjoy <a href="http://steveklabnik.com/">my website</a>.
</p>
</aside>

<section>
  <h1>Recent Posts</h1>
  <ul id="recent_posts">
    
      <li class="post">
        <a href="/2011/10/19/for-science-im-killing-my-cell-phone.html">For Science: I&#8217;m Killing My Cell Phone</a>
      </li>
    
      <li class="post">
        <a href="/2011/10/04/rubinius-is-awesome.html">Rubinius Is Awesome</a>
      </li>
    
      <li class="post">
        <a href="/2011/10/01/github-is-anarchy-for-programmers.html">GitHub Is Anarchy for Programmers</a>
      </li>
    
      <li class="post">
        <a href="/2011/09/28/real-modern-ruby-development.html">(Real) Modern Ruby Development</a>
      </li>
    
      <li class="post">
        <a href="/2011/09/26/im-deleting-my-facebook-tonight.html">I&#8217;m Deleting My Facebook Tonight.</a>
      </li>
    
  </ul>
</section>

<section>
  <h1>Github Repos</h1>
  <ul id="gh_repos">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  
  <a href="https://github.com/steveklabnik">@steveklabnik</a> on Github
  
  <script type="text/javascript">
    $.domReady(function(){
        if (!window.jXHR){
            var jxhr = document.createElement('script');
            jxhr.type = 'text/javascript';
            jxhr.src = '/javascripts/libs/jXHR.js';
            var s = document.getElementsByTagName('script')[0];
            s.parentNode.insertBefore(jxhr, s);
        }

        github.showRepos({
            user: 'steveklabnik',
            count: 0,
            skip_forks: true,
            target: '#gh_repos'
        });
    });
  </script>
  <script src="/javascripts/github.js" type="text/javascript"> </script>
</section>


<section>
  <h1>Latest Tweets</h1>
  <ul id="tweets">
    <li class="loading">Status updating&#8230;</li>
  </ul>
  <script type="text/javascript">
    $.domReady(function(){
      getTwitterFeed("steveklabnik", 4, false);
    });
  </script>
  <script src="/javascripts/twitter.js" type="text/javascript"> </script>
  
    <a href="http://twitter.com/steveklabnik" class="twitter-follow-button" data-width="208px" data-show-count="false">Follow @steveklabnik</a>
  
</section>




  
</aside>

    </div>
  </div>
  <footer><p>
  Copyright &copy; 2011 - Steve Klabnik -
  <span class="credit">Powered by <a href="http://octopress.org">Octopress</a></span>
</p>

</footer>
  
  <script type="text/javascript">
    var _gaq = _gaq || [];
    _gaq.push(['_setAccount', 'UA-10289851-1']);
    _gaq.push(['_trackPageview']);

    (function() {
      var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
      ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
    })();
  </script>


  
  <script type="text/javascript">
    (function() {
      var script = document.createElement('script'); script.type = 'text/javascript'; script.async = true;
      script.src = 'https://apis.google.com/js/plusone.js';
      var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(script, s);
    })();
  </script>


  
  <script type="text/javascript">
    (function(){
      var twitterWidgets = document.createElement('script');
      twitterWidgets.type = 'text/javascript';
      twitterWidgets.async = true;
      twitterWidgets.src = 'http://platform.twitter.com/widgets.js';
      document.getElementsByTagName('head')[0].appendChild(twitterWidgets);
    })();
  </script>


</body>
</html>
